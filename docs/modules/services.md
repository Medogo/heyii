# üîå Module Services

Le module Services contient les int√©grations avec les services externes : STT, LLM, TTS, Vector DB, et T√©l√©phonie.

## Vue d'ensemble

Le module Services est organis√© en sous-modules :
- **STT** : Speech-to-Text (Deepgram)
- **LLM** : Large Language Model (OpenAI)
- **TTS** : Text-to-Speech (ElevenLabs)
- **Vector DB** : Base de donn√©es vectorielle (Qdrant)
- **Telephony** : T√©l√©phonie (Telnyx)

## STT (Speech-to-Text)

### DeepgramSTTClient

**Fichier** : `src/services/stt/deepgram_client.py`

**Responsabilit√©** : Transcription audio en temps r√©el

#### M√©thodes principales

##### `start_streaming(on_transcript_callback)`
D√©marre le streaming STT.

**Param√®tres** :
- `on_transcript_callback` : Callback `(transcript, is_final, confidence) -> None`

**Exemple** :
```python
from src.services.stt.deepgram_client import DeepgramSTTClient

stt_client = DeepgramSTTClient()

async def on_transcript(transcript, is_final, confidence):
    print(f"Transcription: {transcript} (final: {is_final}, conf: {confidence})")

await stt_client.start_streaming(on_transcript)
```

##### `send_audio(audio_chunk: bytes)`
Envoie un chunk audio pour transcription.

##### `close()`
Ferme la connexion STT.

### BaseSTTClient

**Fichier** : `src/services/stt/base.py`

**Responsabilit√©** : Interface abstraite pour STT

Permet d'impl√©menter d'autres fournisseurs STT (Google, Azure, etc.)

## LLM (Large Language Model)

### OpenAIClient

**Fichier** : `src/services/llm/openai_client.py`

**Responsabilit√©** : G√©n√©ration de r√©ponses et extraction de commandes

#### M√©thodes principales

##### `extract_order_items(transcript: str, context: Dict) -> str`
Extrait les produits et quantit√©s d'une transcription.

**Retour** : JSON string avec les produits

**Exemple** :
```python
from src.services.llm.openai_client import OpenAIClient

llm_client = OpenAIClient()

result = await llm_client.extract_order_items(
    "Je voudrais 10 boites de Doliprane et 5 Efferalgan",
    {"conversation_history": [...]}
)
# Retourne: {"products": [{"name": "Doliprane", "quantity": 10, ...}]}
```

##### `generate_response(user_message: str, conversation_history: List) -> str`
G√©n√®re une r√©ponse conversationnelle.

##### `analyze_intent(transcript: str) -> Dict`
Analyse l'intention du message.

### Prompts

**Fichier** : `src/services/llm/prompts.py`

**Responsabilit√©** : Templates de prompts

#### Prompts disponibles

- `SYSTEM_PROMPTS["extraction"]` : Prompt pour extraction de commandes
- `SYSTEM_PROMPTS["dialogue"]` : Prompt pour dialogue conversationnel
- `SYSTEM_PROMPTS["intent_analysis"]` : Prompt pour analyse d'intention

### Functions

**Fichier** : `src/services/llm/functions.py`

**Responsabilit√©** : Sch√©mas de function calling

#### Fonctions disponibles

- `extract_order` : Extraction de produits
- `search_product` : Recherche de produit

### BaseLLMClient

**Fichier** : `src/services/llm/base.py`

**Responsabilit√©** : Interface abstraite pour LLM

## TTS (Text-to-Speech)

### ElevenLabsTTSClient

**Fichier** : `src/services/tts/elevenlabs_client.py`

**Responsabilit√©** : Synth√®se vocale

#### M√©thodes principales

##### `text_to_speech_stream(text: str) -> AsyncGenerator[bytes, None]`
Convertit texte en audio (streaming).

**Exemple** :
```python
from src.services.tts.elevenlabs_client import ElevenLabsTTSClient

tts_client = ElevenLabsTTSClient()

async for audio_chunk in tts_client.text_to_speech_stream("Bonjour"):
    # Envoyer audio_chunk
    pass
```

##### `text_to_speech(text: str) -> bytes`
Convertit texte en audio (complet).

### TTSCache

**Fichier** : `src/services/tts/cache.py`

**Responsabilit√©** : Cache des r√©ponses audio

#### M√©thodes principales

##### `get(text: str, voice_id: str) -> Optional[bytes]`
R√©cup√®re l'audio depuis le cache.

##### `set(text: str, voice_id: str, audio_data: bytes)`
Met en cache l'audio.

**Exemple** :
```python
from src.services.tts.cache import tts_cache

# V√©rifier le cache
cached_audio = await tts_cache.get("Bonjour", "voice_id")
if cached_audio:
    return cached_audio

# G√©n√©rer et mettre en cache
audio = await tts_client.text_to_speech("Bonjour")
await tts_cache.set("Bonjour", "voice_id", audio)
```

### BaseTTSClient

**Fichier** : `src/services/tts/base.py`

**Responsabilit√©** : Interface abstraite pour TTS

## Vector DB

### QdrantClient

**Fichier** : `src/services/vector_db/qcadrant_client.py`

**Responsabilit√©** : Recherche vectorielle de produits

#### M√©thodes principales

##### `search_products(query: str, limit: int = 5) -> List[Dict]`
Recherche s√©mantique de produits.

**Exemple** :
```python
from src.services.vector_db.qcadrant_client import qdrant_client

results = await qdrant_client.search_products("Doliprane", limit=5)
for result in results:
    print(f"{result['name']} - Score: {result['score']}")
```

##### `index_products_batch(products: List[Dict]) -> int`
Indexe une liste de produits.

##### `get_collection_info() -> Dict`
R√©cup√®re les informations de la collection.

### EmbeddingGenerator

**Fichier** : `src/services/vector_db/embeddings.py`

**Responsabilit√©** : G√©n√©ration d'embeddings

#### M√©thodes principales

##### `generate_embedding(text: str) -> List[float]`
G√©n√®re un embedding pour un texte.

##### `generate_embeddings_batch(texts: List[str]) -> List[List[float]]`
G√©n√®re des embeddings en batch.

### ProductIndexer

**Fichier** : `src/services/vector_db/indexer.py`

**Responsabilit√©** : Indexation de produits

#### M√©thodes principales

##### `index_products(products: List[Dict]) -> int`
Indexe une liste de produits.

##### `reindex_all(products: List[Dict]) -> int`
R√©indexe tous les produits.

## Telephony

Les services de t√©l√©phonie utilisent Telnyx. Les r√©f√©rences √† Twilio ont √©t√© supprim√©es.

## Configuration

Les services sont configur√©s dans `src/core/config.py` :

```python
# Deepgram
deepgram_api_key: str
deepgram_model: str = "nova-2"
deepgram_language: str = "fr-FR"

# OpenAI
openai_api_key: str
openai_model: str = "gpt-4o"
openai_temperature: float = 0.3

# ElevenLabs
elevenlabs_api_key: str
elevenlabs_voice_id: str
elevenlabs_model: str = "eleven_turbo_v2_5"

# Qdrant
qdrant_host: str = "localhost"
qdrant_port: int = 6333
qdrant_collection: str = "products"

# Telnyx (remplace Twilio)
telnyx_api_key: str
telnyx_phone_number: str
telnyx_connection_id: str
```

## Gestion des erreurs

Tous les services g√®rent les erreurs avec retry automatique :

- **STT** : Retry sur erreur de connexion
- **LLM** : Retry sur erreur API
- **TTS** : Fallback vers cache si erreur
- **Vector DB** : Retry sur erreur de recherche

## Tests

Les tests du module Services sont dans `tests/unit/test_services/`.

```bash
pytest tests/unit/test_services/ -v
```
